{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt\n",
    "from misc import initialize_weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################U-NET################################\n",
    "\n",
    "\n",
    "#U_net architecture model\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from misc import initialize_weights1\n",
    "\n",
    "\n",
    "class _EncoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout=False):\n",
    "        super(_EncoderBlock, self).__init__()\n",
    "        layers = [\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        if dropout:\n",
    "            layers.append(nn.Dropout())\n",
    "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class _DecoderBlock(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super(_DecoderBlock, self).__init__()\n",
    "        self.decode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, middle_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(middle_channels, middle_channels, kernel_size=3),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.enc1 = _EncoderBlock(3, 64)\n",
    "        self.enc2 = _EncoderBlock(64, 128)\n",
    "        self.enc3 = _EncoderBlock(128, 256)\n",
    "        self.enc4 = _EncoderBlock(256, 512, dropout=True)\n",
    "        self.center = _DecoderBlock(512, 1024, 512)\n",
    "        self.dec4 = _DecoderBlock(1024, 512, 256)\n",
    "        self.dec3 = _DecoderBlock(512, 256, 128)\n",
    "        self.dec2 = _DecoderBlock(256, 128, 64)\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        initialize_weights1(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        center = self.center(enc4)\n",
    "        dec4 = self.dec4(torch.cat([center, F.upsample(enc4, center.size()[2:], mode='bilinear')], 1))\n",
    "        dec3 = self.dec3(torch.cat([dec4, F.upsample(enc3, dec4.size()[2:], mode='bilinear')], 1))\n",
    "        dec2 = self.dec2(torch.cat([dec3, F.upsample(enc2, dec3.size()[2:], mode='bilinear')], 1))\n",
    "        dec1 = self.dec1(torch.cat([dec2, F.upsample(enc1, dec2.size()[2:], mode='bilinear')], 1))\n",
    "        final = self.final(dec1)\n",
    "        x=F.upsample(final, x.size()[2:], mode='bilinear')\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(x)\n",
    "        #x = x.view(-1,512,512)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _DecoderBlock_seg(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_conv_layers):\n",
    "        super(_DecoderBlock_seg, self).__init__()\n",
    "        middle_channels = int(in_channels / 2)\n",
    "        \n",
    "#         print(middle_channels,in_channels)\n",
    "        layers = [\n",
    "            nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        layers += [\n",
    "                      nn.Conv2d(middle_channels, middle_channels, kernel_size=3, padding=1),\n",
    "                      nn.BatchNorm2d(middle_channels),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                  ] * (num_conv_layers - 2)\n",
    "        layers += [\n",
    "            nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "        self.decode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(x)\n",
    "\n",
    "\n",
    "class SegNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True):\n",
    "        super(SegNet, self).__init__()\n",
    "        vgg = models.vgg19_bn(pretrained=True)\n",
    "#         if pretrained:\n",
    "#             vgg.load_state_dict(torch.load(\"vgg19.pth\"))\n",
    "        features = list(vgg.features.children())\n",
    "        self.enc1 = nn.Sequential(*features[0:7])\n",
    "        self.enc2 = nn.Sequential(*features[7:14])\n",
    "        self.enc3 = nn.Sequential(*features[14:27])\n",
    "        self.enc4 = nn.Sequential(*features[27:40])\n",
    "        self.enc5 = nn.Sequential(*features[40:])\n",
    "\n",
    "        self.dec5 = nn.Sequential(\n",
    "            *([nn.ConvTranspose2d(512, 512, kernel_size=2, stride=2)] +\n",
    "              [nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "               nn.BatchNorm2d(512),\n",
    "               nn.ReLU(inplace=True)] * 4)\n",
    "        )\n",
    "        self.dec4 = _DecoderBlock_seg(1024, 256, 4)\n",
    "        self.dec3 = _DecoderBlock_seg(512, 128, 4)\n",
    "        self.dec2 = _DecoderBlock_seg(256, 64, 2)\n",
    "        self.dec1 = _DecoderBlock_seg(128, num_classes, 2)\n",
    "        initialize_weights1(self.dec5, self.dec4, self.dec3, self.dec2, self.dec1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        enc3 = self.enc3(enc2)\n",
    "        enc4 = self.enc4(enc3)\n",
    "        enc5 = self.enc5(enc4)\n",
    "\n",
    "        dec5 = self.dec5(enc5)\n",
    "        dec4 = self.dec4(torch.cat([enc4, dec5], 1))\n",
    "        dec3 = self.dec3(torch.cat([enc3, dec4], 1))\n",
    "        dec2 = self.dec2(torch.cat([enc2, dec3], 1))\n",
    "        dec1 = self.dec1(torch.cat([enc1, dec2], 1))\n",
    "#         x = dec1.view(-1,512,512)\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(dec1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################PSP-NET#####################################\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "\n",
    "from misc import initialize_weights1\n",
    "from misc import Conv2dDeformable\n",
    "#from .config import res101_path\n",
    "\n",
    "\n",
    "class _PyramidPoolingModule(nn.Module):\n",
    "    def __init__(self, in_dim, reduction_dim, setting):\n",
    "        super(_PyramidPoolingModule, self).__init__()\n",
    "        self.features = []\n",
    "        for s in setting:\n",
    "            self.features.append(nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(s),\n",
    "                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(reduction_dim, momentum=.95),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ))\n",
    "        self.features = nn.ModuleList(self.features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        out = [x]\n",
    "        for f in self.features:\n",
    "            out.append(F.upsample(f(x), x_size[2:], mode='bilinear'))\n",
    "        out = torch.cat(out, 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class PSPNet(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=True, use_aux=True):\n",
    "        super(PSPNet, self).__init__()\n",
    "        self.use_aux = use_aux\n",
    "        resnet = models.resnet101(pretrained=True)\n",
    "#         if pretrained:\n",
    "#             resnet.load_state_dict(torch.load(res101_path))\n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1, self.layer2, self.layer3, self.layer4 = resnet.layer1, resnet.layer2, resnet.layer3, resnet.layer4\n",
    "\n",
    "        for n, m in self.layer3.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (2, 2), (2, 2), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "        for n, m in self.layer4.named_modules():\n",
    "            if 'conv2' in n:\n",
    "                m.dilation, m.padding, m.stride = (4, 4), (4, 4), (1, 1)\n",
    "            elif 'downsample.0' in n:\n",
    "                m.stride = (1, 1)\n",
    "\n",
    "        self.ppm = _PyramidPoolingModule(2048, 512, (1, 2, 3, 6))\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(4096, 512, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512, momentum=.95),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(512, num_classes, kernel_size=1)\n",
    "        )\n",
    "\n",
    "        if use_aux:\n",
    "            self.aux_logits = nn.Conv2d(1024, num_classes, kernel_size=1)\n",
    "            initialize_weights1(self.aux_logits)\n",
    "\n",
    "        initialize_weights1(self.ppm, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        if self.training and self.use_aux:\n",
    "            aux = self.aux_logits(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.ppm(x)\n",
    "        x = self.final(x)\n",
    "        if self.training and self.use_aux:\n",
    "            xx = F.upsample(x, x_size[2:], mode='bilinear')\n",
    "            yy = F.upsample(aux, x_size[2:], mode='bilinear')\n",
    "            m = torch.nn.Softmax()\n",
    "            x = m(xx)\n",
    "            y = m(yy)\n",
    "            \n",
    "            return x, y\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "        \n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(single_conv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "############################################U-NET####################################################\n",
    "class U_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=1):\n",
    "        super(U_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(d1)\n",
    "\n",
    "        return x\n",
    "\n",
    "#####################################################Reccurent U-NET###########################################\n",
    "class R2U_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=5,t=2):\n",
    "        super(R2U_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.Upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)\n",
    "\n",
    "        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)\n",
    "        \n",
    "        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)\n",
    "        \n",
    "        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)\n",
    "        \n",
    "        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)\n",
    "        \n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)\n",
    "        \n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.RRCNN1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.RRCNN2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.RRCNN3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.RRCNN4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.RRCNN5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_RRCNN5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_RRCNN4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_RRCNN3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_RRCNN2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(d1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "###############################################Attention-U-NET######################################################\n",
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=5):\n",
    "        super(AttU_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(d1)\n",
    "\n",
    "        return x\n",
    "\n",
    "################################################Reccurent Attention U-NET############################################\n",
    "class R2AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=3,output_ch=5,t=2):\n",
    "        super(R2AttU_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.Upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)\n",
    "\n",
    "        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)\n",
    "        \n",
    "        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)\n",
    "        \n",
    "        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)\n",
    "        \n",
    "        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)\n",
    "        \n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n",
    "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)\n",
    "        \n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n",
    "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.RRCNN1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.RRCNN2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.RRCNN3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.RRCNN4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.RRCNN5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_RRCNN5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_RRCNN4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_RRCNN3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_RRCNN2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        m = torch.nn.Softmax()\n",
    "        x = m(d1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beauty-ubuntu/Desktop/segmentation models/misc.py:20: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(module.weight)\n"
     ]
    }
   ],
   "source": [
    "net1 = UNet(1)\n",
    "net2 = SegNet(1)\n",
    "net3 = PSPNet(1)\n",
    "net4 = U_Net()\n",
    "net5 = R2U_Net()\n",
    "net6 = AttU_Net()\n",
    "net7 = R2AttU_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2351: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:86: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:201: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:274: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:350: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beauty-ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:431: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "input = Variable(torch.randn(1,3,512,512))\n",
    "input2=Variable(torch.randn(2,3,512,512))\n",
    "output1=net1(input)\n",
    "output2=net2(input)\n",
    "output3=net3(input2)\n",
    "output4=net4(input)\n",
    "output5=net5(input)\n",
    "output6=net6(input)\n",
    "output7=net7(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512, 512]) torch.Size([1, 1, 512, 512]) torch.Size([2, 1, 512, 512]) torch.Size([1, 1, 512, 512]) torch.Size([1, 5, 512, 512]) torch.Size([1, 5, 512, 512]) torch.Size([1, 5, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "print(output1.shape, output2.shape, output3[0].shape, output4.shape, output5.shape, output6.shape, output7.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
